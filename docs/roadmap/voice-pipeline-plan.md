# Audio Pipeline Architecture - Voice Integration Plan

## Overview

MidTerm.Voice enables voice interaction with terminal sessions through browser audio streaming and AI providers.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (MidTerm frontend)                                 â”‚
â”‚  â”œâ”€ Terminal UI (xterm.js)                                  â”‚
â”‚  â”œâ”€ Audio capture (Web Audio API)                           â”‚
â”‚  â””â”€ Two WebSocket connections:                              â”‚
â”‚      â”œâ”€ /ws/mux â†’ MidTerm server (terminal I/O)             â”‚
â”‚      â””â”€ /voice â†’ MidTerm.Voice (audio stream)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
           â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MidTerm Server     â”‚â—„â”€â”€â”€â–ºâ”‚  MidTerm.Voice Server           â”‚
â”‚  (mt.exe)           â”‚     â”‚  â”œâ”€ /voice (browser audio)      â”‚
â”‚  â””â”€ Terminal I/O    â”‚     â”‚  â”œâ”€ VoiceAssistant â†’ AI         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â””â”€ WebSocketAudioHardware      â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation Status

### âœ… Phase 1: Server-Side Infrastructure (COMPLETE)

Created `Ai.Tlbx.MidTerm.Voice` project:

| File | Description | Status |
|------|-------------|--------|
| `Ai.Tlbx.MidTerm.Voice.csproj` | Project with VoiceAssistant + OpenAI provider refs | âœ… |
| `WebSockets/WebSocketAudioHardware.cs` | `IAudioHardwareAccess` impl for WebSocket | âœ… |
| `WebSockets/VoiceWebSocketHandler.cs` | `/voice` endpoint handler | âœ… |
| `WebSockets/VoiceJsonContext.cs` | AOT-safe JSON serialization | âœ… |
| `Services/VoiceSessionService.cs` | Configuration (API keys, prompts) | âœ… |
| `Program.cs` | Voice server entry point (port 3000) | âœ… |

**WebSocket Protocol:**
- `{ "type": "start" }` - Start voice session with OpenAI
- `{ "type": "stop" }` - Stop voice session
- Binary frames (browserâ†’server): PCM 16-bit audio from mic
- Binary frames (serverâ†’browser): PCM 16-bit audio from AI
- `{ "type": "config", "sampleRate": 24000 }` - Configure sample rate
- `{ "type": "error", "message": "..." }` - Error notification

### ğŸ”² Phase 2: Browser Audio Code (PENDING)

Need to add audio capture/playback to MidTerm frontend.

**Option A (Quick):** Copy existing JS from VoiceAssistant.Hardware.Web
```
VoiceAssistant/Hardware/Ai.Tlbx.VoiceAssistant.Hardware.Web/wwwroot/js/
â”œâ”€â”€ webAudioAccess.js      â†’ MidTerm/src/static/js/
â””â”€â”€ audio-processor.js     â†’ MidTerm/src/static/js/
```

**Option B (Better, Future):** Create npm package from TypeScript source
- Port JS to TypeScript in VoiceAssistant repo
- Publish as `@ai-tlbx/voice-audio` npm package
- Both MidTerm and Hardware.Web consume from single source

**MidTerm Frontend Tasks:**
| File | Description | Status |
|------|-------------|--------|
| `src/static/js/webAudioAccess.js` | Copy from VoiceAssistant | ğŸ”² |
| `src/static/js/audio-processor.js` | Copy from VoiceAssistant | ğŸ”² |
| `src/ts/modules/voice.ts` | WebSocket + audio glue | ğŸ”² |
| `src/static/index.html` | Add voice button | ğŸ”² |
| `Settings/MidTermSettings.cs` | Add VoiceServiceUrl | ğŸ”² |

### ğŸ”² Phase 3: Terminal Integration (PENDING)

Connect voice AI to terminal operations:

| Task | Description | Status |
|------|-------------|--------|
| Terminal tools | Add tools for VoiceAssistant to execute terminal commands | ğŸ”² |
| Session bridge | Connect VoiceAssistant to MidTerm session API | ğŸ”² |
| Context awareness | Voice AI understands current terminal state | ğŸ”² |

### ğŸ”² Phase 4: Single Source Refactor (FUTURE)

Port to TypeScript npm package for DRY audio code:

```
VoiceAssistant/
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ voice-audio/                    # npm: @ai-tlbx/voice-audio
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ webAudioAccess.ts       # Port from JS
â”‚       â”‚   â”œâ”€â”€ audioProcessor.ts       # Port from JS (worklet)
â”‚       â”‚   â””â”€â”€ index.ts                # Exports
â”‚       â””â”€â”€ package.json
â”‚
â””â”€â”€ Hardware/
    â””â”€â”€ Ai.Tlbx.VoiceAssistant.Hardware.Web/
        â””â”€â”€ wwwroot/js/voice-audio.umd.js  # Built from npm package
```

## Audio Processing Details

The browser audio code (from VoiceAssistant.Hardware.Web) includes:
- 48kHz capture with echo cancellation, noise suppression, auto gain
- De-esser, compressor, anti-aliasing filters
- Provider-specific downsampling: 16kHz (Google) or 24kHz (OpenAI/xAI)
- AudioWorklet processor for efficient real-time processing

## Configuration

**Environment Variables:**
- `OPENAI_API_KEY` - OpenAI API key for voice provider

**appsettings.json:**
```json
{
  "OpenAI": {
    "ApiKey": "sk-..."
  },
  "Voice": {
    "SystemPrompt": "You are a helpful assistant...",
    "MidTermServerUrl": "https://localhost:2000"
  },
  "Port": 3000
}
```

## Running the Voice Server

```bash
cd Ai.Tlbx.MidTerm.Voice
dotnet run
# Listening on http://0.0.0.0:3000
# WebSocket endpoint: /voice
```

## Testing

1. Run MidTerm on localhost:2000
2. Run MidTerm.Voice on localhost:3000
3. Connect browser WebSocket to ws://localhost:3000/voice
4. Send `{ "type": "start" }` to begin session
5. Send binary PCM audio frames
6. Receive binary PCM responses from AI
